{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test REST APIs\n",
    "\n",
    "This notebook provides example commands for interacting with the AI-Q Research Assistant backend APIs. Before using this notebook, ensure you have followed the deployment guide to deploy the NVIDIA RAG blueprint and the AI-Q Reseearch Assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Update the following environment variables with the IP address of the server where you are running these tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import subprocess\n",
    "os.environ[\"INFERENCE_ORIGIN\"]=\"http://your-server-ip:8051\"\n",
    "os.environ[\"RAG_BASE_URL\"]= \"http://your-server-ip\"\n",
    "\n",
    "#Helper function to measure the time taken to execute the query\n",
    "import time\n",
    "\n",
    "def run_curl(curl_cmd):\n",
    "    print(f\"\\nExecuting: {curl_cmd}\")\n",
    "    start = time.time()\n",
    "\n",
    "    # Run the command\n",
    "    result = subprocess.run(curl_cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "    duration = time.time() - start\n",
    "    print(\"\\n--- Response ---\")\n",
    "    print(result.stdout.strip())\n",
    "\n",
    "    if result.stderr:\n",
    "        print(\"\\n--- Errors ---\")\n",
    "        print(result.stderr.strip())\n",
    "\n",
    "    print(f\"\\n⏱️  Completed in {duration:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Collection\n",
    "\n",
    "The following command creates a collection. For any unsuccessful query executions, check the logs in the following containers for troubleshooting:\n",
    " - `docker logs aira-nginx`\n",
    " - `docker logs ingestor-server`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "run_curl(f\"\"\"curl -s -X POST {os.environ['INFERENCE_ORIGIN']}/v1/collections \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '[\"test_collection\"]'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload a File to the Collection\n",
    "\n",
    "The following command uploads a file to the test collection. For any unsuccessful query executions, check the logs in the following container for troubleshooting:\n",
    "\n",
    "- `docker logs ingestor-server`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " # Update the directory based on where you have downloaded the notebook\n",
    " \n",
    " run_curl(f\"\"\"curl -X POST {os.environ['INFERENCE_ORIGIN']}/v1/documents \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -F 'documents=@/home/ubuntu/aiq-research-assistant/notebooks/simple.pdf' \\\n",
    "  -F 'data={{\\\"collection_name\\\": \\\"test_collection\\\"}}'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask Questions About Your PDF to RAG\n",
    "\n",
    "The following endpoint allows you to ask questions about your PDF to RAG. For any errors, check the logs of the container below for troubleshooting:\n",
    "\n",
    "- `docker logs rag-server`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " run_curl(f\"\"\"curl -X POST \"{os.environ['RAG_BASE_URL']}:8081/v1/generate\" \\\n",
    "        -H 'accept: application/json' \\\n",
    "        -H 'Content-Type: application/json' \\\n",
    "        -d '{{ \n",
    "            \"messages\": [ \n",
    "                {{ \\\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"Give me a summary of the topic\" \n",
    "                }} \n",
    "            ], \n",
    "            \"use_knowledge_base\": true, \n",
    "            \"temperature\": 0.2, \n",
    "            \"top_p\": 0.7, \n",
    "            \"max_tokens\": 1024, \n",
    "            \"reranker_top_k\": 10, \n",
    "            \"vdb_top_k\": 100, \n",
    "            \"vdb_endpoint\": \"http://milvus:19530\", \n",
    "            \"collection_name\": \"test_collection\", \n",
    "            \"enable_query_rewriting\": false, \n",
    "            \"enable_reranker\": false, \n",
    "            \"enable_guardrails\": false, \n",
    "            \"enable_citations\": true, \n",
    "            \"stop\": [] \n",
    "            }}'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Sample Research Plan\n",
    "\n",
    "This command replicates the first part of the AI-Q Research Assistant workflow by creating a research plan given a collection, report topic, and report organization prompt.Troubleshooting Steps:\n",
    "- If no response is given, check the nginx logs by running `docker logs aira-nginx`.\n",
    "- If a 403 or 404 is given, check that the nemotron API key and model configuration in the aira config.yaml file are correct. Try making a direct shell request to the nemotron model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " run_curl(f\"\"\"curl -s -X POST {os.environ['INFERENCE_ORIGIN']}/generate_query/stream \\\n",
    "        -H 'accept: application/json' \\\n",
    "        -H 'Content-Type: application/json' \\\n",
    "        -d '{{\n",
    "            \"topic\": \"An awesome report\",\n",
    "            \"report_organization\": \"A comprehensive report with an introduction, body, and conclusion with a witty joke\",\n",
    "            \"num_queries\": 3,\n",
    "            \"llm_name\": \"nemotron\"\n",
    "    }}'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Report\n",
    "\n",
    "The following command generates a report based on the topic, report organization, and queries. If you encounter any errors while running it, you can troubleshoot by checking the logs of the following container:\n",
    "- `docker logs aira-backend`\n",
    "\n",
    "*Note that the following command outputs the response.txt file instead of printing the output to the console.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "run_curl(f\"\"\"curl -s -X POST {os.environ['INFERENCE_ORIGIN']}/generate_summary/stream \\\n",
    "        -H 'accept: application/json' \\\n",
    "        -H 'Content-Type: application/json' \\\n",
    "        -d '{{\n",
    "        \"topic\": \"An awesome report\",\n",
    "        \"report_organization\": \"A comprehensive report with an introduction, body, and conclusion with a witty joke\",\n",
    "        \"queries\": [\n",
    "            {{\n",
    "            \"query\": \"overview of the topic\",\n",
    "            \"report_section\": \"overview\",\n",
    "            \"rationale\": \"key information\"\n",
    "            }},\n",
    "            {{\n",
    "            \"query\": \"big mac ingredients\",\n",
    "            \"report_section\": \"web search\",\n",
    "            \"rationale\": \"just for fun\"\n",
    "            }}\n",
    "        ],\n",
    "        \"search_web\": true,\n",
    "        \"rag_collection\": \"test_collection\",\n",
    "        \"reflection_count\": 1,\n",
    "        \"llm_name\": \"nemotron\"\n",
    "    }}' > report.txt 2>&1\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask Q&A\n",
    "\n",
    "The following command replicates the human in the loop interactions available in the web frontend including editing the report plan, the draft report, and doing Q&A. \n",
    "\n",
    "Troubleshooting tips:You can troubleshoot by checking the logs of the container below:\n",
    "- `docker logs aira-backend`\n",
    "\n",
    "The Q&A endpoint relies on the instruct_llm configured in the AI-Q Research Assistant config.yaml file. Verify this configuration and attempt a direct shell command against that llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "run_curl(f\"\"\"curl -s -X POST {os.environ['INFERENCE_ORIGIN']}/artifact_qa/stream \\\n",
    "        -H \"accept: application/json\" \\\n",
    "        -H \"Content-Type: application/json\" \\\n",
    "        -d '{{\n",
    "            \"additional_context\": \"\",\n",
    "            \"artifact\": \"\\\\n\\\\n# An extensive report on the topic of the users favorite PDF\",\n",
    "            \"chat_history\": [],\n",
    "            \"question\": \"edit the title to something more professional\",\n",
    "            \"rewrite_mode\": \"entire\",\n",
    "            \"use_internet\": false,\n",
    "            \"rag_collection\": \"test_collection\"\n",
    "        }}'\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
